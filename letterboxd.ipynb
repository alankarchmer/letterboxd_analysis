{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwekPf0c7FL5"
      },
      "outputs": [],
      "source": [
        "!pip install PyHive\n",
        "!pip install thrift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zooF-bi7b7o"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import create_engine\n",
        "from TCLIService.ttypes import TOperationState\n",
        "from pyhive import hive\n",
        "import requests\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEaAU-a6O8CE"
      },
      "outputs": [],
      "source": [
        "protocol = 'https' # Connection protocol can be 'http' or 'https'\n",
        "\n",
        "# username - Use 'token' as the username when connecting using a Timbr token, otherwise its the user name.\n",
        "\n",
        "user_name = 'token'\n",
        "\n",
        "# userpass - Should be the token value if using a token as a username, otherwise its the user's password.\n",
        "\n",
        "user_pass = '***'\n",
        "\n",
        "# hostname - The IP / Hostname of the Timbr server (not necessarily the hostname of the Timbr platform).\n",
        "\n",
        "hostname = 'azure-env2.timbr.ai'\n",
        "\n",
        "# port - Timbr default port 11000\n",
        "\n",
        "port = '443'\n",
        "\n",
        "# ontology - the ontology / knowledge graph to connect to.\n",
        "\n",
        "ontology = 'csv_tables'\n",
        "\n",
        "\n",
        "\n",
        "engine = create_engine(f\"hive+{protocol}://{user_name}@{ontology}:{user_pass}@{hostname}:{port}\")\n",
        "\n",
        "conn = engine.connect()\n",
        "\n",
        "\n",
        "\n",
        "#query = \"SHOW CONCEPTS\"\n",
        "\n",
        "\n",
        "\n",
        "query = \"\"\"\n",
        "\n",
        "SELECT\n",
        "\n",
        "       `review`, `person_id_person_film[person_film].rating` AS \"rating\"\n",
        "\n",
        "FROM `dtimbr`.`viewing_with_ids`\n",
        "\n",
        "WHERE `review` IS NOT NULL AND `person_id_person_film[person_film].rating` IS NOT NULL\n",
        "\n",
        "AND `person_id_person_film[person_film].films_id` = `films_id`\n",
        "\n",
        "LIMIT 100\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "dbapi_conn = engine.raw_connection()\n",
        "\n",
        "cursor = dbapi_conn.cursor()\n",
        "\n",
        "cursor.execute(query, async_=True)\n",
        "\n",
        "status = cursor.poll().operationState\n",
        "\n",
        "while status in (TOperationState.INITIALIZED_STATE, TOperationState.RUNNING_STATE):\n",
        "\n",
        "    status = cursor.poll().operationState\n",
        "\n",
        "\n",
        "\n",
        "print(status)\n",
        "\n",
        "cursor._arraysize = 100\n",
        "\n",
        "results = cursor.fetchall()\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58ko2-ZsWiog"
      },
      "outputs": [],
      "source": [
        "def executeQuery(url, ontology, token, query):\n",
        "\n",
        "    if not url.endswith(\"/\"):\n",
        "\n",
        "       url += \"/\"\n",
        "\n",
        "    post_data = {'ontology_name': ontology, 'query': query}\n",
        "\n",
        "    headers = {'Content-Type': 'application/json', 'x-api-key': token}\n",
        "\n",
        "    response = requests.post(url + \"timbr/api/query/\", headers = headers, json = post_data, verify = False)\n",
        "\n",
        "    response_data = response.json()\n",
        "\n",
        "    if response_data['status'] == 'success':\n",
        "\n",
        "        df = pd.DataFrame(response_data['data'])\n",
        "\n",
        "        return df\n",
        "\n",
        "    else:\n",
        "\n",
        "        raise Exception(\"Error in request: \" + response_data['data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPECo7SCbUk7"
      },
      "outputs": [],
      "source": [
        "url = \"https://azure-env2.timbr.ai\" # http://<hostname>:<port> or https://<hostname>:<port> for example (your environment): https://azure-env2.timbr.ai/\n",
        "\n",
        "ontology = \"csv_tables\" # ontology name, for example: timbr_imdb\n",
        "\n",
        "token = \"tk_ab44fcf5cb043c12f6a85e10bcb2cbbe37c3736b3158cb7cfb64a942f0b09352\" # The value of your user token (can be found in the homepage in the user profile box or run the query “show token” in SQL Editor)\n",
        "\n",
        "query = \"SHOW CONCEPT_RELATIONSHIPS\" # The SQL query you wish to run\n",
        "\n",
        "\n",
        "\n",
        "response = executeQuery(url, ontology, token, query)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZA9JpBxa9QM"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"SELECT\n",
        "       `review`, `person_id_person_film[person_film].rating` AS \"rating\"\n",
        "FROM `dtimbr`.`viewing_with_ids`\n",
        "WHERE `review` IS NOT NULL AND `person_id_person_film[person_film].rating` IS NOT NULL\n",
        "AND `person_id_person_film[person_film].films_id` = `films_id`\n",
        "LIMIT 100000\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Prepare the data\n",
        "data_nlp = executeQuery(url, ontology, token, query)\n",
        "\n",
        "data_nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89Bm0LweXYbg"
      },
      "outputs": [],
      "source": [
        "# Prepare the data\n",
        "review_data = data_nlp[['review']]\n",
        "#review_data['sentiment'] = \"\"\n",
        "\n",
        "# Create a Tokenizer object to convert the text data into sequences of integers\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(review_data[\"review\"])\n",
        "X = tokenizer.texts_to_sequences(review_data[\"review\"])\n",
        "\n",
        "# Define the maximum length of the sequences\n",
        "max_len = max([len(x) for x in X])\n",
        "\n",
        "# Pad the sequences to ensure that all the sequences have the same length\n",
        "X = pad_sequences(X, maxlen=max_len)\n",
        "\n",
        "# Assign sentiment labels to reviews\n",
        "data_nlp[\"sentiment\"] = data_nlp['rating']\n",
        "\n",
        "# define y\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "data_nlp[\"sentiment\"] = le.fit_transform(data_nlp[\"sentiment\"])\n",
        "y = data_nlp[\"sentiment\"]/10.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqM2LqCfl3b6"
      },
      "outputs": [],
      "source": [
        "#review_data['sentiment'] = prediction.T[0] > 0.35\n",
        "#review_data['sentiment'] = review_data['sentiment'].apply(lambda x: 'positive' if x else 'negative')\n",
        "review_data['prediction'] = pd.cut(pd.Series(prediction.T[0]),10, labels=[1,2,3,4,5,6,7,8,9,10])\n",
        "\n",
        "# Output the results\n",
        "#review_data.drop('sentiment',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkEnWIgWg_Gg"
      },
      "outputs": [],
      "source": [
        "review_data['ratings'] = data_nlp['rating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5ivWjyQkMCc"
      },
      "outputs": [],
      "source": [
        "review_data.ratings.groupby(review_data.prediction).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhU_7dFtnBUD"
      },
      "outputs": [],
      "source": [
        "review_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Sm_aqJK-glZ"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=32, input_length=max_len))\n",
        "model.add(LSTM(units=32))\n",
        "#model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=16)\n",
        "\n",
        "# Evaluate the performance of the model on the testing data\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test loss: {loss:.3f}\")\n",
        "print(f\"Test accuracy: {accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6l0L6qY6g9E"
      },
      "outputs": [],
      "source": [
        "model.save('/content/letterboxd_rnn_split_e20_b16.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p6fjkVY4k3l"
      },
      "outputs": [],
      "source": [
        "files.download('letterboxd_rnn_split_e20_b16.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBxeWTZoxf3e"
      },
      "outputs": [],
      "source": [
        "loaded_model = load_model('/content/letterboxd_rnn_split_e20_b16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bdic9G_x7kX8"
      },
      "outputs": [],
      "source": [
        "loaded_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apT0E0bmsgnp"
      },
      "outputs": [],
      "source": [
        "model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6qOmf3Fty8q"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DQa9XpKuLsK"
      },
      "outputs": [],
      "source": [
        "qs = pd.Series(predictions.T[0]).quantile([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x4TAJ2ewlV1"
      },
      "outputs": [],
      "source": [
        "qs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lws5LwXWuRKu"
      },
      "outputs": [],
      "source": [
        "review_data.loc[y_test.index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EffAv8WEvWwg"
      },
      "outputs": [],
      "source": [
        "test_p = model.predict(X_test).T[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vceMr8CFw5os"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({'y':y_train,'p':predictions.T[0]}).plot.scatter(x='p',y='y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST-u43t5w7hf"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({'y':y_test,'p':test_p}).plot.scatter(x='p',y='y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWLSpPfYxw6W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
